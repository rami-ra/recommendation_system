{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nearest Neighbour (item-based)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, nearest neighbor algorithm will be implemented to create a recommendation system for movies. \n",
    "\n",
    "This notebook contains part of the documentation. The full details of the algorithm can be found in the accompanying report found in the repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will install the dependencies. We need to install numpy, sklearn and scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /home/rami/.local/lib/python3.8/site-packages (1.20.1)\n",
      "Requirement already satisfied: scipy in /home/rami/.local/lib/python3.8/site-packages (1.6.0)\n",
      "Requirement already satisfied: sklearn in /home/rami/.local/lib/python3.8/site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /home/rami/.local/lib/python3.8/site-packages (from sklearn) (0.24.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/rami/.local/lib/python3.8/site-packages (from scikit-learn->sklearn) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/rami/.local/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy scipy sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we import the required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import operator\n",
    "import multiprocessing\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "from typing import Any, Dict, List\n",
    "from multiprocessing import Process\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to retrieve the train, test and validation datasets as well as the constructed utility matrix, we will the MatrixMaker class.\n",
    "The class is responsible for constructing the matrices and storing them to save computation time. Make sure to delete this files when changing the ratings file.\n",
    "The full functionality of the class can be found in the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files found, returning the matrices.\n"
     ]
    }
   ],
   "source": [
    "from matrix_maker import MatrixMaker\n",
    "\n",
    "data_retriever = MatrixMaker()\n",
    "(train_set, test_set, validation_set, utility_matrix) = data_retriever.make_matrices(remake=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the Logger class to log meta data about the execution of the program so it can be used later to analyse the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from logger import Logger\n",
    "\n",
    "OPERATION = \"nearest_neighbour\"\n",
    "logger = Logger(OPERATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will get the program configuration from a configuration file. This is done to separate the parameters of the program from the program itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = None\n",
    "try:\n",
    "    config = json.load(open(\"config.json\", \"r\"))\n",
    "except Exception as e:\n",
    "    print(e.__doc__)\n",
    "    print(\"Check if config file exists and is in good order\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **predict_nearestneighnor_mp** function uses collaborative filtering to predicted the ratings of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_nearestneighbor_mp(matrix: np.ndarray, predictions: Any, knn: int, return_dict: Any, pid: int, global_average: float, users_bias: Dict, movies_bias: Dict) -> None:\n",
    "    \"\"\"Use nearest neighbor collaborative filtering to predict ratings. Built with multiprocessing in mind.\n",
    "\n",
    "    Args:\n",
    "        matrix (ee.ndarray): The utility matrix\n",
    "        predictions (Any): Rating for a movie by a user to be made\n",
    "        knn (int): The number of nearest neighbors to be used in the algorithm\n",
    "        return_dict (Any): A return dictonnary used to return the results of a Process\n",
    "        pid (int): The id of the current process\n",
    "        global_average (float): The average rating of the train set\n",
    "        users_bias (Dict): A dictionary containing the biases of all users.\n",
    "        movies_bias (Dict): A dictionary containing the biases of all movies.\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for index in range(len(predictions)):\n",
    "        userp = int(predictions[index, 0])-1\n",
    "        moviep = int(predictions[index, 1])-1\n",
    "        sim = get_similarities(userp, moviep, matrix)\n",
    "        sim.sort(key=operator.itemgetter(1), reverse=True)\n",
    "        simk = np.asarray(sim[0:knn])\n",
    "        rating = 0\n",
    "        sum = 0\n",
    "        for i in simk:\n",
    "            m = int(i[0])\n",
    "            s = i[1]\n",
    "            if(np.isnan(s)):\n",
    "                continue\n",
    "            sum = sum + s\n",
    "            rating = rating + s * (matrix[m, userp] - get_baseline_estimate(m, userp, global_average, users_bias, movies_bias))\n",
    "        if (sum == 0):\n",
    "            rating = get_baseline_estimate(moviep, userp, global_average, users_bias, movies_bias)\n",
    "        else:\n",
    "            rating = get_baseline_estimate(moviep, userp, global_average, users_bias, movies_bias) + (rating / sum)\n",
    "        result.append((index+1, rating))\n",
    "    return_dict[pid] = result\n",
    "    print(\"finished job in process \"+str(pid+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **get_similarities** function finds the similarities of an entry to other entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarities(user_id: int, movie_id: int, matrix: np.ndarray) -> List[float]:\n",
    "    \"\"\"Calculate the similarity of the current movie to other movies.\n",
    "\n",
    "    Args:\n",
    "        user_id (int): The index of the user\n",
    "        movie_id (int): The index of the movie\n",
    "        matrix (ee.ndarray): The utility matrix\n",
    "\n",
    "    Returns:\n",
    "        List[float]: The list of similarities\n",
    "    \"\"\"\n",
    "    sim = list()\n",
    "    a = matrix[movie_id, :]\n",
    "    anan = np.argwhere(np.isnan(a)).transpose().flatten()\n",
    "    for i in range(matrix.shape[0]):\n",
    "        if(i == movie_id or np.isnan(matrix[i, user_id])): \n",
    "            continue\n",
    "        b = matrix[i, :]\n",
    "        bnan = np.argwhere(np.isnan(b)).transpose().flatten()\n",
    "        delx = np.unique(np.concatenate((anan, bnan)))\n",
    "        ax = np.delete(a, delx)\n",
    "        bx = np.delete(b, delx)\n",
    "        if(len(ax) <= 1 or len(bx) <= 1): \n",
    "            continue\n",
    "        corr, p_value = pearsonr(ax, bx)\n",
    "        # Handling of PearsonRConstantInputWarning \n",
    "        if np.isnan(corr):\n",
    "            corr = 0.0\n",
    "        sim.append((i, corr))\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **get_baseline_estimate** function is used to get the baseline estimate (Global + Local) for a particular entry. This is done to get better estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_baseline_estimate(movie_id: int, user_id: int, global_average: float, users_bias: Dict, movies_bias: Dict) -> float:\n",
    "    \"\"\"Calculate the baseline estimate for a particular movie and user\n",
    "\n",
    "    Args:\n",
    "        movie_id (int): The index of the movie\n",
    "        user_id (int): The index of the user\n",
    "        global_average (float): The average rating of the train set\n",
    "        users_bias (Dict): A dictionary containing the biases of all users.\n",
    "        movies_bias (Dict): A dictionary containing the biases of all movies.\n",
    "\n",
    "    Returns:\n",
    "        float: the baseline estimate\n",
    "    \"\"\"    \n",
    "    return global_average+users_bias[user_id]+movies_bias[movie_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **get_biases** function will be used to calculate the biases for movies and users. This is needed to add the local effects for the final predicted ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_biases(utility_matrix: np.ndarray, global_average: float) -> (Dict, Dict):\n",
    "    \"\"\"Calculate biases for movies and users.\n",
    "\n",
    "    Args:\n",
    "        utility_matrix (ee.ndarray): The utility matrix\n",
    "        global_average (float): The average rating for the train set\n",
    "\n",
    "    Returns:\n",
    "        (Dict, Dict): Dictionaries for user and movie biases\n",
    "    \"\"\"    \n",
    "    # Calculate the user biases\n",
    "    users_bias = dict()\n",
    "    for i in range(utility_matrix.shape[1]):\n",
    "        m = np.nanmean(utility_matrix[:, i])\n",
    "        # Handling of Mean of empty slice runtime warning\n",
    "        if(np.isnan(m)):\n",
    "            users_bias[i] = 0.0\n",
    "        else: \n",
    "            users_bias[i] = m - global_average\n",
    "    \n",
    "    # Calculate the movies biases\n",
    "    movies_bias = dict()\n",
    "    for i in range(utility_matrix.shape[0]):\n",
    "        m = np.nanmean(utility_matrix[i, :])\n",
    "        # Handling of Mean of empty slice runtime warning\n",
    "        if(np.isnan(m)):\n",
    "            movies_bias[i] = 0.0\n",
    "        else:\n",
    "            movies_bias[i] = m - global_average\n",
    "\n",
    "    return (users_bias, movies_bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **calculate_RMSE** function is used to calculate the Root Mean Squared Error which is used to find the accuracy of the algorithm on test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_RMSE(results: List[float], prediction_set: np.ndarray) -> float:\n",
    "    \"\"\"Calculate the RMSE between the results and prediction set.\n",
    "\n",
    "    Args:\n",
    "        results (List[float]): The list of predicted results\n",
    "        test_set (ee.ndarray): The test set \n",
    "\n",
    "    Returns:\n",
    "        float: The RMSE between the results and test set\n",
    "    \"\"\"\n",
    "    expected = prediction_set[:, 2].flatten()\n",
    "    assert len(expected) == len(results)\n",
    "    return math.sqrt(mean_squared_error(expected, results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the main function. Multiprocessing will be used to speed up the computation by creating multiple processes. The number of processes is defined in the config file.\n",
    "\n",
    "**Note:** Multiprocessing doesn't work well with Jupyter Notebook when running on Windows. There might be some issues when the program is executed on Windows. It is preferred to use Linux to run this notebook. Setting the number of processes to 1 could also help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-1facae9144f8>:24: RuntimeWarning: Mean of empty slice\n",
      "  m = np.nanmean(utility_matrix[i, :])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting process: 1\n",
      "starting process: 2\n",
      "starting process: 3\n",
      "starting process: 4\n",
      "starting process: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rami/.local/lib/python3.8/site-packages/scipy/stats/stats.py:3913: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n",
      "/home/rami/.local/lib/python3.8/site-packages/scipy/stats/stats.py:3913: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting process: 6\n",
      "starting process: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rami/.local/lib/python3.8/site-packages/scipy/stats/stats.py:3913: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting process: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rami/.local/lib/python3.8/site-packages/scipy/stats/stats.py:3913: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n",
      "/home/rami/.local/lib/python3.8/site-packages/scipy/stats/stats.py:3913: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n",
      "/home/rami/.local/lib/python3.8/site-packages/scipy/stats/stats.py:3913: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n",
      "/home/rami/.local/lib/python3.8/site-packages/scipy/stats/stats.py:3913: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n",
      "/home/rami/.local/lib/python3.8/site-packages/scipy/stats/stats.py:3913: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished job in process 7\n",
      "finished job in process 8\n",
      "finished job in process 5\n",
      "finished job in process 1\n",
      "finished job in process 4\n",
      "finished job in process 6\n",
      "finished job in process 2\n",
      "finished job in process 3\n",
      "--- 1037.5483438968658 seconds ---\n",
      "--- rmse: 31.390324472551338 ---\n",
      "log: nearest_neighbour, 1037.5483438968658, 31.390324472551338, 8, , 9, 0.005, 20, 12, 0.05, False, 5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    # Number of processes\n",
    "    num_processes = config[\"number_processes\"]\n",
    "    # Number of neighbours\n",
    "    num_neighbours = config[\"number_neighbours\"]\n",
    "    (number_users, number_movies, max_ratings, max_timestamp) = np.max(train_set, axis=0)\n",
    "    number_predictions = len(test_set)\n",
    "    number_ratings = len(train_set)\n",
    "    global_average = train_set.mean(axis=0)[2]\n",
    "    # test_set = test_set[0:1000, :]\n",
    "    (users_bias, movies_bias) = get_biases(utility_matrix, global_average)\n",
    "\n",
    "    start_time = time.time()\n",
    "    # Create multiple chunks so each chunk can be assigned to a different process\n",
    "    chunks = np.array_split(test_set, num_processes)\n",
    "    manager = multiprocessing.Manager()\n",
    "    return_dict = manager.dict()\n",
    "    processes = []\n",
    "    # Create the multiple processes\n",
    "    for i in range(num_processes):\n",
    "        print(\"starting process: \"+str(i+1))\n",
    "        process = Process(target=predict_nearestneighbor_mp, args=(utility_matrix, chunks[i], num_neighbours, return_dict, i, global_average, users_bias, movies_bias))\n",
    "        processes.append(process)\n",
    "        process.start()\n",
    "    for j in processes:\n",
    "        j.join()\n",
    "    \n",
    "    # Retrieve the results from the multiple processes.\n",
    "    pr1 = return_dict.items()\n",
    "    pr1.sort(key=operator.itemgetter(0), reverse=False)\n",
    "    pr2 = list()\n",
    "    for i in pr1:\n",
    "        pr2.append(i[1])\n",
    "    flattened_list = [y for x in pr2 for y in x]\n",
    "    print(flattened_list)\n",
    "    results = [p[1] for p in flattened_list]\n",
    "    total_time = (time.time() - start_time)\n",
    "    rmse = calculate_RMSE(results, test_set)\n",
    "    print(\"--- \" + str(total_time) + \" seconds ---\")\n",
    "    print(\"--- rmse: \" + str(rmse) + \" ---\")\n",
    "    logger.save(total_time, rmse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
