{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Factors Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, a latent factor model will be implemented to create a recommendation system for movies. \n",
    "\n",
    "This notebook contains part of the documentation. The full details of the algorithm can be found in the accompanying report found in the repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will install the dependencies. We need to install numpy and optuna. Optuna will be used for hyper-parameter optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy sklearn optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we import the required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "import os\n",
    "import operator\n",
    "import math\n",
    "import time\n",
    "from typing import Any, Dict, List\n",
    "import json\n",
    "import optuna\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to retrieve the train, test and validation datasets as well as the constructed utility matrix, we will the MatrixMaker class.\n",
    "The class is responsible for constructing the matrices and storing them to save computation time. Make sure to delete this files when changing the ratings file.\n",
    "The full functionality of the class can be found in the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matrix_maker import MatrixMaker\n",
    "\n",
    "data_retriever = MatrixMaker()\n",
    "(train_set, test_set, validation_set, utility_matrix) = data_retriever.make_matrices(remake=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the Logger class to log meta data about the execution of the program so it can be used later to analyse the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from logger import Logger\n",
    "\n",
    "OPERATION = \"latent_factors\"\n",
    "logger = Logger(OPERATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will get the program configuration from a configuration file. This is done to separate the parameters of the program from the program itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = None\n",
    "try:\n",
    "    config = json.load(open(\"config.json\", \"r\"))\n",
    "except Exception as e:\n",
    "    print(e.__doc__)\n",
    "    print(\"Check if config file exists and is in good order\")\n",
    "\n",
    "hyper_optimization = bool(config[\"hyper_optimization\"])\n",
    "hyper_epoch = config[\"hyper_epoch\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **get_biases** function will be used to calculate the biases for movies and users. This is needed to add the local effects for the final predicted ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_biases(utility_matrix: np.ndarray, global_average: float) -> (Dict, Dict):\n",
    "    \"\"\"Calculate biases for movies and users.\n",
    "\n",
    "    Args:\n",
    "        utility_matrix (ee.ndarray): The utility matrix\n",
    "        global_average (float): The average rating for the train set\n",
    "\n",
    "    Returns:\n",
    "        (Dict, Dict): Dictionaries for user and movie biases\n",
    "    \"\"\"    \n",
    "    # Calculate the user biases\n",
    "    users_bias = dict()\n",
    "    for i in range(utility_matrix.shape[1]):\n",
    "        m = np.nanmean(utility_matrix[:, i])\n",
    "        if(np.isnan(m)):\n",
    "            users_bias[i] = 0.0\n",
    "        else: \n",
    "            users_bias[i] = m - global_average\n",
    "    \n",
    "    # Calculate the movies biases\n",
    "    movies_bias = dict()\n",
    "    for i in range(utility_matrix.shape[0]):\n",
    "        m = np.nanmean(utility_matrix[i, :])\n",
    "        if(np.isnan(m)):\n",
    "            movies_bias[i] = 0.0\n",
    "        else:\n",
    "            movies_bias[i] = m - global_average\n",
    "\n",
    "    return (users_bias, movies_bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now use the **get_biases** function to get the biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(number_users, number_movies, max_ratings, max_timestamp) = np.max(train_set, axis=0)\n",
    "number_users = int(number_users)\n",
    "number_movies = int(number_movies)\n",
    "number_predictions = len(test_set)\n",
    "number_ratings = len(train_set)\n",
    "global_average = train_set.mean(axis=0)[2]\n",
    "\n",
    "(init_users_bias, init_movies_bias) = get_biases(utility_matrix, global_average)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **build_latent_factors** will create the matrices q and p through stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_latent_factors(latent_factors: int, train_epoch: int, alpha: float, regularization: float, movies_bias: Dict, users_bias: Dict) -> (np.ndarray, np.ndarray):\n",
    "    \"\"\"Calculate biases for movies and users.\n",
    "\n",
    "    Args:\n",
    "        latent_factors (int): The number of latent factors to build the Q and P matrices\n",
    "        train_epoch (int): The number of training cycles\n",
    "        alpha (float): The learning rate\n",
    "        regularization (float): The regularization factor\n",
    "        movies_bias (Dict): A dictionary containing the biases of all movies.\n",
    "        users_bias (Dict): A dictionary containing the biases of all users.\n",
    "\n",
    "    Returns:\n",
    "        (np.ndarray, np.ndarray): The Q and P matrices respectively\n",
    "    \"\"\"    \n",
    "    #Intiallize random matrices q and p\n",
    "    q = np.random.rand(number_movies, latent_factors)\n",
    "    p = np.random.rand(latent_factors, number_users)\n",
    "    #Perform stochastic gradient descent to get matrices q and p\n",
    "    for e in range(train_epoch):\n",
    "        print(\"Iteration \"+str(e+1)+ \" out of \"+str(train_epoch))\n",
    "        for i in range(number_movies):\n",
    "            for j in range(number_users):\n",
    "                if(np.isnan(utility_matrix[i, j])): continue\n",
    "                current_rating = predict(p, q, i, j, movies_bias, users_bias)\n",
    "                difference = utility_matrix[i, j] - current_rating\n",
    "                movies_bias[i] = movies_bias[i] + (alpha * (difference-(regularization*movies_bias[i])))\n",
    "                users_bias[j] = users_bias[j] + (alpha * (difference-(regularization*users_bias[j])))\n",
    "                q[i, :] = q[i, :] + (alpha*((difference*p[:, j])-(regularization*q[i, :])))\n",
    "                p[:, j] = p[:, j] + (alpha*((difference*q[i, :])-(regularization*p[:, j])))\n",
    "    return (q, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **predict_results** function will loop through the entries and produces a list of the predicted ratings. It will either loop through the test set or validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_results(q: np.ndarray, p: np.ndarray, movies_bias: Dict, users_bias: Dict, prediction_set: np.ndarray) -> List[float]:\n",
    "    \"\"\"Calculate the predicted ratings for the provided data set.\n",
    "\n",
    "    Args:\n",
    "        q (np.ndarray): The Q matrix (movies x latent factors)\n",
    "        p (np.ndarray): The P matrix (latent factors x users)\n",
    "        movies_bias (Dict): A dictionary containing the biases of all movies.\n",
    "        users_bias (Dict): A dictionary containing the biases of all users.\n",
    "        prediction_set (np.ndarray): The data set that the predictions need to be made from\n",
    "\n",
    "    Returns:\n",
    "        List[float]: The list of the predicted ratings.\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for index in range(len(prediction_set)):\n",
    "        userp = int(prediction_set[index, 0])-1\n",
    "        moviep = int(prediction_set[index, 1])-1\n",
    "        rating = predict(p, q, moviep, userp, movies_bias, users_bias)\n",
    "        result.append(rating)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **predict** function calculated the rating for a single entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(p: np.ndarray, q: np.ndarray, i: int, j: int, movies_bias: Dict, users_bias: Dict) -> float:\n",
    "    \"\"\"Calculate the predicted entry for a single entry.\n",
    "\n",
    "    Args:\n",
    "        q (np.ndarray): The Q matrix (movies x latent factors)\n",
    "        p (np.ndarray): The P matrix (latent factors x users)\n",
    "        i (int): The movie index\n",
    "        j (int): The user index\n",
    "        movies_bias (Dict): A dictionary containing the biases of all movies.\n",
    "        users_bias (Dict): A dictionary containing the biases of all users.\n",
    "\n",
    "    Returns:\n",
    "        float: The predicted rating.\n",
    "    \"\"\"\n",
    "    return global_average + movies_bias[i] + users_bias[j] + np.dot(q[i, :], p[:, j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **calculate_RMSE** function is used to calculate the Root Mean Squared Error which is used to find the accuracy of the algorithm on a data set (test/validation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_RMSE(results: List[float], prediction_set: np.ndarray) -> float:\n",
    "    \"\"\"Calculate the RMSE between the results and prediction set.\n",
    "\n",
    "    Args:\n",
    "        results (List[float]): The list of predicted results\n",
    "        test_set (ee.ndarray): The test set \n",
    "\n",
    "    Returns:\n",
    "        float: The RMSE between the results and test set\n",
    "    \"\"\"\n",
    "    expected = prediction_set[:, 2].flatten()\n",
    "    assert len(expected) == len(results)\n",
    "    return math.sqrt(mean_squared_error(expected, results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **objective** function is used by Optuna for hyper-parameter optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.trial.Trial) -> float:\n",
    "    \"\"\"Used by Optuna for hyper parameter optimization.\n",
    "    Calculates the RMSE for a particular set of hyper parameters.\n",
    "\n",
    "    Args:\n",
    "        trial (optuna.trial.Trial): The Trial object that Optuna uses.\n",
    "\n",
    "    Returns:\n",
    "        float: The RMSE of the model built using the hyperparameters on the validation set.\n",
    "    \"\"\"    \n",
    "    latent_factors = trial.suggest_int(\"latent_factors\", 7, 18)\n",
    "    train_epoch = trial.suggest_int(\"train_epoch\", 200, 600, 50)\n",
    "    alpha = trial.suggest_float(\"alpha\", 0.01, 0.02)\n",
    "    regularization = trial.suggest_float(\"regularization\", 0.045, 0.85)\n",
    "    movies_bias = init_movies_bias.copy()\n",
    "    users_bias = init_users_bias.copy() \n",
    "    (q, p) = build_latent_factors(latent_factors, train_epoch, alpha, regularization, movies_bias, users_bias)\n",
    "    results = predict_results(q, p, movies_bias, users_bias, validation_set)\n",
    "    return calculateRMSE(results, validation_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the main function. If hyperoptimization is on, then the program will use Optuna to optimize the hyper-parameters, otherwise the Q and P matrices will be contructed and the predictions will be made using the paramters from the config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    movies_bias = init_movies_bias.copy()\n",
    "    users_bias = init_users_bias.copy() \n",
    "    if not hyper_optimization:\n",
    "        latent_factors = config[\"latent_factors\"]\n",
    "        train_epoch = config[\"train_epoch\"]\n",
    "        alpha = config[\"alpha\"]\n",
    "        regularization = config[\"regularization_factor\"]\n",
    "        start_time = time.time()\n",
    "        (q, p) = build_latent_factors(latent_factors, train_epoch, alpha, regularization, movies_bias, users_bias)\n",
    "        results = predict_results(q, p, movies_bias, users_bias, test_set)\n",
    "\n",
    "        # Calculate the time taken and RMSE and save to the log file\n",
    "        time = time.time() - start_time\n",
    "        rmse = calculate_RMSE(results, test_set)\n",
    "        print(\"--- %s seconds ---\" % (time))\n",
    "        print(rmse)\n",
    "        logger.save(time, rmse)\n",
    "\n",
    "    else:\n",
    "        # Start an Optuna study for hyper parameter optimization\n",
    "        study = optuna.create_study()\n",
    "        print(\"start of hyperoptimization\")\n",
    "        study.optimize(objective, n_trials=hyper_epoch, n_jobs=-1)\n",
    "        print(\"end of hyperoptimization\")\n",
    "\n",
    "        # Retrieve the best parameters found\n",
    "        latent_factors = study.best_params[\"latent_factors\"]\n",
    "        train_epoch = study.best_params[\"train_epoch\"]\n",
    "        alpha = study.best_params[\"alpha\"]\n",
    "        regularization = study.best_params[\"regularization_factor\"]\n",
    "\n",
    "        # Modify the logger object to the new parameters\n",
    "        logger.latent_factors = latent_factors\n",
    "        logger.train_epoch = train_epoch\n",
    "        logger.alpha = alpha\n",
    "        logger.regularization = regularization\n",
    "\n",
    "        # Calculate the time taken and RMSE and save to the log file\n",
    "        start_time = time.time()\n",
    "        (q, p) = build_latent_factors(latent_factors, train_epoch, alpha, regularization, movies_bias, users_bias)\n",
    "        results = predict_results(q, p, movies_bias, users_bias, test_set)\n",
    "        total_time = time.time() - start_time\n",
    "        rmse = calculate_RMSE(test_set, results)\n",
    "        print(\"--- \" + str(total_time) + \" seconds ---\")\n",
    "        print(\"--- rmse: \" + str(rmse) + \" ---\")\n",
    "        logger.save(total_time, rmse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}